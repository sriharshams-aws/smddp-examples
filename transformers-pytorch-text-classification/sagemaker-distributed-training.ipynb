{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hugging Face Transformers with `Pytorch` \n",
    "### Text Classification Example using vanilla `Pytorch`, `Transformers`, `Datasets`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Welcome to this end-to-end multilingual Text-Classification example using PyTorch. In this demo, we will use the Hugging Faces `transformers` and `datasets` library together with `Pytorch` to fine-tune a multilingual transformer for text-classification. This example is a derived version of the [text-classificiaton.ipynb](https://github.com/philschmid/transformers-pytorch-text-classification/blob/main/text-classification.ipynb) notebook and uses Amazon SageMaker for distributed training. In the [text-classificiaton.ipynb](https://github.com/philschmid/transformers-pytorch-text-classification/blob/main/text-classification.ipynb) we showed how to fine-tune `distilbert-base-multilingual-cased` on the `amazon_reviews_multi` dataset for `sentiment-analysis`. This dataset has over 1.2 million data points, which is huge. Running training would take on 1x NVIDIA V100 takes around 6,5h for `batch_size` 16, which is quite long.\n",
    "\n",
    "To scale and accelerate our training we will use [Amazon SageMaker](https://aws.amazon.com/de/sagemaker/), which provides two strategies for [distributed training](https://huggingface.co/docs/sagemaker/train#distributed-training), [data parallelism](https://huggingface.co/docs/sagemaker/train#data-parallelism) and model parallelism. Data parallelism splits a training set across several GPUs, while [model parallelism](https://huggingface.co/docs/sagemaker/train#model-parallelism) splits a model across several GPUs. We are going to use [SageMaker Data Parallelism](https://aws.amazon.com/blogs/aws/managed-data-parallelism-in-amazon-sagemaker-simplifies-training-on-large-datasets/), which has been built into the [Trainer](https://huggingface.co/transformers/main_classes/trainer.html) API. To be able use data-parallelism we only have to define the `distribution` parameter in our `HuggingFace` estimator.\n",
    "\n",
    "I moved the \"training\" part of the [text-classificiaton.ipynb](https://github.com/philschmid/transformers-pytorch-text-classification/blob/main/text-classification.ipynb) notebook into a separate training script [train.py](./scripts/train.py), which accepts the same hyperparameter and can be run on Amazon SageMaker using the `HuggingFace` estimator. \n",
    "\n",
    "Our goal is to decrease the training duration by scaling our global/effective batch size from 16 up to 128, which is 8x bigger than before. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.17.0-py3-none-any.whl (3.8 MB)\n",
      "     |████████████████████████████████| 3.8 MB 23.6 MB/s            \n",
      "\u001b[?25hCollecting datasets\n",
      "  Downloading datasets-1.18.4-py3-none-any.whl (312 kB)\n",
      "     |████████████████████████████████| 312 kB 116.3 MB/s            \n",
      "\u001b[?25hCollecting tensorboard\n",
      "  Downloading tensorboard-2.8.0-py3-none-any.whl (5.8 MB)\n",
      "     |████████████████████████████████| 5.8 MB 115.5 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from transformers) (2.26.0)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from transformers) (3.3.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from transformers) (5.4.1)\n",
      "Collecting sacremoses\n",
      "  Downloading sacremoses-0.0.47-py2.py3-none-any.whl (895 kB)\n",
      "     |████████████████████████████████| 895 kB 130.5 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from transformers) (21.0)\n",
      "Collecting huggingface-hub<1.0,>=0.1.0\n",
      "  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n",
      "     |████████████████████████████████| 67 kB 12.4 MB/s             \n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from transformers) (1.18.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from transformers) (4.62.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from transformers) (2021.11.2)\n",
      "Collecting tokenizers!=0.11.3,>=0.11.1\n",
      "  Downloading tokenizers-0.11.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.5 MB)\n",
      "     |████████████████████████████████| 6.5 MB 109.7 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: importlib-metadata in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from transformers) (4.8.2)\n",
      "Requirement already satisfied: multiprocess in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from datasets) (0.70.12.2)\n",
      "Requirement already satisfied: aiohttp in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from datasets) (3.8.1)\n",
      "Collecting xxhash\n",
      "  Downloading xxhash-3.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
      "     |████████████████████████████████| 212 kB 125.8 MB/s            \n",
      "\u001b[?25hCollecting responses<0.19\n",
      "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Collecting fsspec[http]>=2021.05.0\n",
      "  Downloading fsspec-2022.2.0-py3-none-any.whl (134 kB)\n",
      "     |████████████████████████████████| 134 kB 127.5 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: pyarrow!=4.0.0,>=3.0.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: dill in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from datasets) (0.3.4)\n",
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from datasets) (1.3.4)\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.6.0-py2.py3-none-any.whl (156 kB)\n",
      "     |████████████████████████████████| 156 kB 121.7 MB/s            \n",
      "\u001b[?25hCollecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.3.6-py3-none-any.whl (97 kB)\n",
      "     |████████████████████████████████| 97 kB 17.3 MB/s             \n",
      "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from tensorboard) (0.37.0)\n",
      "Collecting grpcio>=1.24.3\n",
      "  Downloading grpcio-1.44.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.3 MB)\n",
      "     |████████████████████████████████| 4.3 MB 118.5 MB/s            \n",
      "\u001b[?25hCollecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "     |████████████████████████████████| 781 kB 82.8 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: setuptools>=41.0.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from tensorboard) (58.5.3)\n",
      "Collecting absl-py>=0.4\n",
      "  Downloading absl_py-1.0.0-py3-none-any.whl (126 kB)\n",
      "     |████████████████████████████████| 126 kB 129.5 MB/s            \n",
      "\u001b[?25hCollecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
      "     |████████████████████████████████| 4.9 MB 107.9 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: werkzeug>=0.11.15 in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from tensorboard) (2.0.2)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from tensorboard) (3.19.1)\n",
      "Requirement already satisfied: boto3 in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from datasets) (1.21.11)\n",
      "Requirement already satisfied: botocore in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from datasets) (1.24.11)\n",
      "Requirement already satisfied: fsspec in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from datasets) (2021.4.0)\n",
      "Requirement already satisfied: s3fs in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from datasets) (2021.4.0)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from absl-py>=0.4->tensorboard) (1.16.0)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-5.0.0-py3-none-any.whl (9.1 kB)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard) (4.7.2)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "     |████████████████████████████████| 155 kB 19.5 MB/s            \n",
      "\u001b[?25hCollecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.6.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from packaging>=20.0->transformers) (3.0.5)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from requests->transformers) (3.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from requests->transformers) (2.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from requests->transformers) (1.26.8)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from aiohttp->datasets) (20.3.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from aiohttp->datasets) (5.2.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from aiohttp->datasets) (4.0.1)\n",
      "Requirement already satisfied: asynctest==0.13.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from aiohttp->datasets) (0.13.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from aiohttp->datasets) (1.7.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from boto3->datasets) (0.10.0)\n",
      "Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from boto3->datasets) (0.5.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from botocore->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from pandas->datasets) (2021.3)\n",
      "Collecting s3fs\n",
      "  Downloading s3fs-2022.2.0-py3-none-any.whl (26 kB)\n",
      "Collecting aiobotocore~=2.1.0\n",
      "  Downloading aiobotocore-2.1.2.tar.gz (58 kB)\n",
      "     |████████████████████████████████| 58 kB 14.0 MB/s            \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: joblib in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from sacremoses->transformers) (1.1.0)\n",
      "Requirement already satisfied: click in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from sacremoses->transformers) (8.0.3)\n",
      "  Downloading aiobotocore-2.1.1.tar.gz (57 kB)\n",
      "     |████████████████████████████████| 57 kB 11.0 MB/s             \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Downloading aiobotocore-2.1.0.tar.gz (54 kB)\n",
      "     |████████████████████████████████| 54 kB 5.1 MB/s             \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hINFO: pip is looking at multiple versions of xxhash to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting xxhash\n",
      "  Downloading xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243 kB)\n",
      "     |████████████████████████████████| 243 kB 113.3 MB/s            \n",
      "\u001b[?25hINFO: pip is looking at multiple versions of sacremoses to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting sacremoses\n",
      "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
      "     |████████████████████████████████| 895 kB 75.4 MB/s            \n",
      "\u001b[?25hINFO: pip is looking at multiple versions of s3fs to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting s3fs\n",
      "  Downloading s3fs-2022.1.0-py3-none-any.whl (25 kB)\n",
      "  Downloading s3fs-2021.11.1-py3-none-any.whl (25 kB)\n",
      "Collecting aiobotocore~=2.0.1\n",
      "  Downloading aiobotocore-2.0.1.tar.gz (54 kB)\n",
      "     |████████████████████████████████| 54 kB 6.8 MB/s              \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting s3fs\n",
      "  Downloading s3fs-2021.11.0-py3-none-any.whl (25 kB)\n",
      "  Downloading s3fs-2021.10.1-py3-none-any.whl (26 kB)\n",
      "Collecting aiobotocore~=1.4.1\n",
      "  Downloading aiobotocore-1.4.2.tar.gz (52 kB)\n",
      "     |████████████████████████████████| 52 kB 2.8 MB/s              \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting s3fs\n",
      "  Downloading s3fs-2021.10.0-py3-none-any.whl (26 kB)\n",
      "  Downloading s3fs-2021.9.0-py3-none-any.whl (26 kB)\n",
      "  Downloading s3fs-2021.8.1-py3-none-any.whl (26 kB)\n",
      "  Downloading s3fs-2021.8.0-py3-none-any.whl (26 kB)\n",
      "  Downloading s3fs-2021.7.0-py3-none-any.whl (25 kB)\n",
      "  Downloading s3fs-2021.6.1-py3-none-any.whl (25 kB)\n",
      "  Downloading s3fs-2021.6.0-py3-none-any.whl (24 kB)\n",
      "  Downloading s3fs-2021.5.0-py3-none-any.whl (24 kB)\n",
      "  Downloading s3fs-0.6.0-py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: aiobotocore>=1.0.1 in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from s3fs->datasets) (1.3.0)\n",
      "Collecting aiobotocore>=1.0.1\n",
      "  Downloading aiobotocore-2.0.0.tar.gz (52 kB)\n",
      "     |████████████████████████████████| 52 kB 3.3 MB/s              \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Downloading aiobotocore-1.4.1.tar.gz (52 kB)\n",
      "     |████████████████████████████████| 52 kB 2.1 MB/s             \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Downloading aiobotocore-1.4.0.tar.gz (51 kB)\n",
      "     |████████████████████████████████| 51 kB 697 kB/s             \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Downloading aiobotocore-1.3.3.tar.gz (50 kB)\n",
      "     |████████████████████████████████| 50 kB 12.7 MB/s            \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Downloading aiobotocore-1.3.2.tar.gz (49 kB)\n",
      "     |████████████████████████████████| 49 kB 12.2 MB/s            \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Downloading aiobotocore-1.3.1.tar.gz (48 kB)\n",
      "     |████████████████████████████████| 48 kB 11.0 MB/s            \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Downloading aiobotocore-1.2.2.tar.gz (48 kB)\n",
      "     |████████████████████████████████| 48 kB 10.6 MB/s            \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Downloading aiobotocore-1.2.1.tar.gz (48 kB)\n",
      "     |████████████████████████████████| 48 kB 1.2 MB/s             \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Downloading aiobotocore-1.2.0.tar.gz (47 kB)\n",
      "     |████████████████████████████████| 47 kB 9.2 MB/s             \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Downloading aiobotocore-1.1.2-py3-none-any.whl (45 kB)\n",
      "     |████████████████████████████████| 45 kB 6.6 MB/s             \n",
      "\u001b[?25hRequirement already satisfied: aioitertools>=0.5.1 in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from aiobotocore>=1.0.1->s3fs->datasets) (0.8.0)\n",
      "Requirement already satisfied: wrapt>=1.10.10 in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from aiobotocore>=1.0.1->s3fs->datasets) (1.12.1)\n",
      "  Downloading aiobotocore-1.1.1-py3-none-any.whl (45 kB)\n",
      "     |████████████████████████████████| 45 kB 7.6 MB/s              \n",
      "\u001b[?25h  Downloading aiobotocore-1.1.0-py3-none-any.whl (43 kB)\n",
      "     |████████████████████████████████| 43 kB 4.9 MB/s              \n",
      "\u001b[?25h  Downloading aiobotocore-1.0.7-py3-none-any.whl (42 kB)\n",
      "     |████████████████████████████████| 42 kB 3.5 MB/s              \n",
      "\u001b[?25h  Downloading aiobotocore-1.0.6-py3-none-any.whl (42 kB)\n",
      "     |████████████████████████████████| 42 kB 2.3 MB/s              \n",
      "\u001b[?25h  Downloading aiobotocore-1.0.5-py3-none-any.whl (42 kB)\n",
      "     |████████████████████████████████| 42 kB 2.4 MB/s              \n",
      "\u001b[?25h  Downloading aiobotocore-1.0.4-py3-none-any.whl (41 kB)\n",
      "     |████████████████████████████████| 41 kB 1.3 MB/s              \n",
      "\u001b[?25h  Downloading aiobotocore-1.0.3-py3-none-any.whl (40 kB)\n",
      "     |████████████████████████████████| 40 kB 13.3 MB/s            \n",
      "\u001b[?25h  Downloading aiobotocore-1.0.2-py3-none-any.whl (40 kB)\n",
      "     |████████████████████████████████| 40 kB 13.9 MB/s            \n",
      "\u001b[?25h  Downloading aiobotocore-1.0.1-py3-none-any.whl (40 kB)\n",
      "     |████████████████████████████████| 40 kB 14.3 MB/s            \n",
      "\u001b[?25hCollecting s3fs\n",
      "  Downloading s3fs-0.5.2-py3-none-any.whl (22 kB)\n",
      "  Downloading s3fs-0.5.1-py3-none-any.whl (21 kB)\n",
      "  Downloading s3fs-0.5.0-py3-none-any.whl (21 kB)\n",
      "  Downloading s3fs-0.4.2-py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/ec2-user/anaconda3/envs/pytorch_latest_p37/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard) (0.4.8)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.2.0-py3-none-any.whl (151 kB)\n",
      "     |████████████████████████████████| 151 kB 118.2 MB/s            \n",
      "\u001b[?25hInstalling collected packages: pyasn1-modules, oauthlib, fsspec, cachetools, xxhash, responses, requests-oauthlib, huggingface-hub, google-auth, tokenizers, tensorboard-plugin-wit, tensorboard-data-server, sacremoses, s3fs, markdown, grpcio, google-auth-oauthlib, datasets, absl-py, transformers, tensorboard\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2021.4.0\n",
      "    Uninstalling fsspec-2021.4.0:\n",
      "      Successfully uninstalled fsspec-2021.4.0\n",
      "  Attempting uninstall: s3fs\n",
      "    Found existing installation: s3fs 2021.4.0\n",
      "    Uninstalling s3fs-2021.4.0:\n",
      "      Successfully uninstalled s3fs-2021.4.0\n",
      "Successfully installed absl-py-1.0.0 cachetools-5.0.0 datasets-1.18.4 fsspec-2022.2.0 google-auth-2.6.0 google-auth-oauthlib-0.4.6 grpcio-1.44.0 huggingface-hub-0.4.0 markdown-3.3.6 oauthlib-3.2.0 pyasn1-modules-0.2.8 requests-oauthlib-1.3.1 responses-0.18.0 s3fs-0.4.2 sacremoses-0.0.47 tensorboard-2.8.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tokenizers-0.11.6 transformers-4.17.0 xxhash-3.0.0\n"
     ]
    }
   ],
   "source": [
    "#!pip install sagemaker \n",
    "!pip install transformers datasets tensorboard datasets[s3] --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup & Configuration\n",
    "\n",
    "In this step we will define global configurations and parameters, which are used across the whole end-to-end fine-tuning proccess, e.g. `tokenizer` and `model` we will use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker role arn: arn:aws:iam::423047559695:role/service-role/AmazonSageMaker-ExecutionRole-20210428T080830\n",
      "sagemaker bucket: sagemaker-us-west-2-423047559695\n",
      "sagemaker session region: us-west-2\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note: The execution role is only available when running a notebook within SageMaker (SageMaker Notebook Instances or Studio). If you run `get_execution_role` in a notebook not on SageMaker, expect a region error._\n",
    "\n",
    "You can comment in the cell below and provide a an IAM Role name with SageMaker permissions to setup your environment out side of SageMaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sagemaker\n",
    "# import boto3\n",
    "# import os\n",
    "\n",
    "# os.environ[\"AWS_DEFAULT_REGION\"]=\"your-region\"\n",
    "\n",
    "# # This ROLE needs to exists with your associated AWS Credentials and needs permission for SageMaker\n",
    "# ROLE_NAME='role-name-of-your-iam-role-with-right-permissions'\n",
    "\n",
    "# iam_client = boto3.client('iam')\n",
    "# role = iam_client.get_role(RoleName=ROLE_NAME)['Role']['Arn']\n",
    "# sess = sagemaker.Session()\n",
    "\n",
    "# print(f\"sagemaker role arn: {role}\")\n",
    "# print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "# print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example are we going to fine-tune the [distilbert-base-multilingual-cased](https://huggingface.co/distilbert-base-multilingual-cased) a multilingual DistilBERT model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"distilbert-base-multilingual-cased\"\n",
    "\n",
    "# name for our repository on the hub\n",
    "model_name = model_id.split(\"/\")[-1] if \"/\" in model_id else model_id\n",
    "repo_name = f\"{model_name}-sentiment\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset & Pre-processing\n",
    "\n",
    "As Dataset we will use the [amazon_reviews_multi](https://huggingface.co/datasets/amazon_reviews_multi) a multilingual text-classification. The dataset contains reviews in English, Japanese, German, French, Chinese and Spanish, collected between November 1, 2015 and November 1, 2019. Each record in the dataset contains the review text, the review title, the star rating, an anonymized reviewer ID, an anonymized product ID and the coarse-grained product category (e.g. ‘books’, ‘appliances’, etc.) The corpus is balanced across stars, so each star rating constitutes 20% of the reviews in each language.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_id=\"amazon_reviews_multi\"\n",
    "dataset_config=\"all_languages\"\n",
    "\n",
    "seed=33"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load the `amazon_reviews_multi` dataset, we use the `load_dataset()` method from the 🤗 Datasets library.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "794b9776109048e68fa099285afb9327",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/2.74k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d1b06a481f94978bbb5f9c1b8be8bb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/3.62k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset amazon_reviews_multi/all_languages (download: 610.66 MiB, generated: 364.83 MiB, post-processed: Unknown size, total: 975.49 MiB) to /home/ec2-user/.cache/huggingface/datasets/amazon_reviews_multi/all_languages/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "346dd1c5676c4645a2eed3b407390e0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cefbb73df4f4dc49261f429373214a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/90.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08bc25b247fa42f89f0235c0bd4ce229",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/82.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9e735a0bcb74c17b41885f20892dea8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/77.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eaa83394d5e342978376ace8c196c752",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/81.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d179cbf8f4084a30b1785227b752573e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/169M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9b3f73099ce4022b2287fe089c11a4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/109M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22704227c644465fb3d41a13fe49ad2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4948d81c16545308c80c9e20360e4b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40ee2140f0e74138aa91554e5809719d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/2.25M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17d72bdd55df487b9ce05d2f2609f7b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/2.06M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ed24f3299924582b7d3a6c96f30e79d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.93M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dff4c7daab6442b58927275b6a6d9967",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/2.02M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25e7facb48e64fbf9b8b0a521a5cee21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/4.19M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c47b7c7643b422ebdb98db79aceeaef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/2.70M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1624075846de4ed1a71882c5828c28e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "198e7fa5f24841a097aef17cb2790488",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17a2cd2c36ee4aed9f2c800c29addd7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/2.26M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d24e42447e864df78296c851756cc0a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/2.05M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "976880f37da84572a0e135334dd09b0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.94M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b930e8652fd4d12a6fea231576fc73d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/2.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27a4891472924285a6b1d7d2a2183574",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/4.21M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "355333e2af0c4c2b825aa3a5814d50d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/2.73M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ca93b92aeb3419ebd2b82a93cb33593",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2fc96a55a45497db5c9c59bf1778e01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "659330b9f56c420a9179745f1e326ae6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b7a2895d0354654abade652b254d3c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset amazon_reviews_multi downloaded and prepared to /home/ec2-user/.cache/huggingface/datasets/amazon_reviews_multi/all_languages/1.0.0/724e94f4b0c6c405ce7e476a6c5ef4f87db30799ad49f765094cf9770e0f7609. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "382f7157a5a24ff4a8ff81784786ea1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(dataset_id,dataset_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing & Tokenization\n",
    "\n",
    "The [amazon_reviews_multi](https://huggingface.co/datasets/amazon_reviews_multi) has 5 classes (`stars`) to match those into a `sentiment-analysis` task we will map those star ratings to the following classes `labels`:\n",
    "* `[1-2]`: `Negative`\n",
    "* `[3]`: `Neutral`\n",
    "* `[4-5]`: `Positive`\n",
    "\n",
    "Those `labels` can be later used to create a user friendly output after we fine-tuned our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c59694df80a64e2896b2229611a9a668",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0ex [00:00, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "820706fc568f4b55aff15435027ae6b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0ex [00:00, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f23b4ce43054e5aaea8bde7294cf007",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0ex [00:00, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f15b13c4ad24149880eae712481bda0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/120 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59408958d8d0420bb3d4269baf153611",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8658a8f9feaa4fd488447844fe6fa615",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'labels': ClassLabel(num_classes=3, names=['negative', 'neutral', 'positive'], id=None),\n",
       " 'review_body': Value(dtype='string', id=None)}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import ClassLabel\n",
    "\n",
    "def map_start_to_label(review):\n",
    "  if review[\"stars\"] < 3:\n",
    "    review[\"stars\"] = 0\n",
    "  elif review[\"stars\"] == 3:\n",
    "    review[\"stars\"] = 1\n",
    "  else: \n",
    "    review[\"stars\"] = 2\n",
    "  return review\n",
    "\n",
    "# convert 1-5 star reviews to 0,1,2\n",
    "dataset = dataset.map(map_start_to_label)\n",
    "\n",
    "# convert feature from Value to ClassLabel\n",
    "class_feature =  ClassLabel(names=['negative','neutral', 'positive'])\n",
    "dataset = dataset.cast_column(\"stars\", class_feature)\n",
    "\n",
    "# rename our target column to labels\n",
    "dataset = dataset.rename_column(\"stars\",\"labels\")\n",
    "\n",
    "# drop columns that are not needed\n",
    "dataset = dataset.remove_columns(['review_id', 'product_id', 'reviewer_id', 'review_title', 'language', 'product_category'])\n",
    "\n",
    "dataset[\"train\"].features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we prepare the dataset for training. Lets take a quick look at the class distribution of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<AxesSubplot:title={'center':'labels'}>]], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = dataset[\"train\"].to_pandas()\n",
    "\n",
    "df.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Distribution is not perfect, but lets give it a try and improve on this later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train our model we need to convert our \"Natural Language\" to token IDs. This is done by a 🤗 Transformers Tokenizer which will tokenize the inputs (including converting the tokens to their corresponding IDs in the pretrained vocabulary). If you are not sure what this means check out [chapter 6](https://huggingface.co/course/chapter6/1?fw=tf) of the Hugging Face Course.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bfaa49ed29b4116910889040c81807c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdf84b82fd1743f8938a1d1092b4ecbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/466 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "118ca9fa8da641b9b253aeb948b9e33d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/972k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b426543637174c388caab2112a5c8e4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.87M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally we add the `truncation=True` and `max_length=512` to align the length and truncate texts that are bigger than the maximum size allowed by the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca239e9692f944868c7b55044c1e9b7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1200 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f83d6a4faa24e2980da4f5dd1eb8735",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a5d68d2e1ad4cc78a9b990f56c269c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'labels': ClassLabel(num_classes=3, names=['negative', 'neutral', 'positive'], id=None),\n",
       " 'review_body': Value(dtype='string', id=None),\n",
       " 'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None),\n",
       " 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None)}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def process(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"review_body\"], truncation=True, max_length=512\n",
    "    )\n",
    "    return tokenized_inputs\n",
    "\n",
    "tokenized_datasets = dataset.map(process, batched=True)\n",
    "tokenized_datasets[\"train\"].features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can start our distributed Training, we need to upload our already pre-processed dataset to Amazon S3. Therefore we will use the built-in utils of `datasets`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import botocore\n",
    "from datasets.filesystems import S3FileSystem\n",
    "\n",
    "s3 = S3FileSystem()  \n",
    "\n",
    "# save train_dataset to s3\n",
    "training_input_path = f's3://{sess.default_bucket()}/{dataset_id}/train'\n",
    "tokenized_datasets[\"train\"].save_to_disk(training_input_path, fs=s3)\n",
    "\n",
    "# save validation_dataset to s3\n",
    "eval_input_path = f's3://{sess.default_bucket()}/{dataset_id}/test'\n",
    "tokenized_datasets[\"validation\"].save_to_disk(eval_input_path, fs=s3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating an Estimator and start a training job\n",
    "\n",
    "\n",
    "Last step before we can start our managed training is to define our Hyperparameters, create our sagemaker `HuggingFace` estimator and configure distributed training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "# hyperparameters, which are passed into the training job\n",
    "hyperparameters={\n",
    "    'model_id':'distilbert-base-multilingual-cased',           \n",
    "    'epochs': 3,                                    \n",
    "    'per_device_train_batch_size': 16,                         \n",
    "    'per_device_eval_batch_size': 16,                          \n",
    "    'learning_rate': 3e-5*8,                          \n",
    "    'fp16': True,            \n",
    "    # logging & evaluation strategie\n",
    "    'strategy':'steps',\n",
    "    'steps':100,\n",
    "    'save_total_limit':2,\n",
    "    'load_best_model_at_end':True,\n",
    "    'metric_for_best_model':\"f1\",\n",
    "}\n",
    "\n",
    "# configuration for running training on smdistributed Data Parallel\n",
    "distribution = {'smdistributed':{'dataparallel':{ 'enabled': True }}}\n",
    "\n",
    "# create the Estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point          = 'train.py',        \n",
    "    source_dir           = './scripts',       \n",
    "    instance_type        = 'ml.p3.16xlarge',   \n",
    "    instance_count       = 2, \n",
    "    role                 = role,              \n",
    "    transformers_version = '4.12',            \n",
    "    pytorch_version      = '1.9',             \n",
    "    py_version           = 'py38',            \n",
    "    hyperparameters      = hyperparameters,   \n",
    "    distribution         = distribution\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since, we are using SageMaker Data Parallelism our total_batch_size will be per_device_train_batch_size * n_gpus.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a data input dictonary with our uploaded s3 uris\n",
    "data = {\n",
    "    'train': training_input_path,\n",
    "    'eval': eval_input_path\n",
    "}\n",
    "\n",
    "# starting the train job with our uploaded datasets as input\n",
    "# setting wait to False to not expose the HF Token\n",
    "huggingface_estimator.fit(data,wait=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "interpreter": {
   "hash": "ec1370a512a4612a2908be3c3c8b0de1730d00dc30104daff827065aeaf438b7"
  },
  "kernelspec": {
   "display_name": "conda_pytorch_latest_p37",
   "language": "python",
   "name": "conda_pytorch_latest_p37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
